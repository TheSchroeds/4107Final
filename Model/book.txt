Started with creating a sequential dense model, loss was in the millions and acc 0
This was with no pre-processing or dim reduction

After I tried with the same model after normalizing all inputs/outputs on the range 0-1
255 is the max a stat can be so that is easy to normalize
Leaving us with normalizing usage, which we can do by totaling all usages for the top 60 pokemon
and then dividing each individual's total by the overall total.
This obviously lowered loss as we were now delaing with much smallers values, but we still had 0 acc

Throughout this we realized our inputs were incredibly sparse, 575 inputs with the vast majority being 0s
We figured this was a major players in our complete lack of accuracy and thus we began experimenting with autoencoders

We began by creating an autoencoder which would only encode the sparse areas of our inputs, since we did the one-hot encoding, we knew
that all of those values would be sparse. As the stats for all pokemon will always be filled, we decided to avoid using our encoder
to reduce their dimentionality as we wanted to ensure those values remained unchanged.

Now as we had identified a possible problem, we wanted to create the best autoencoder we could to ensure our dimentionaly reduction
did not result in a large loss of data quality. Thus we continued experiementing and iterating on our encoder until we were happy
with the resutls. 

After we had tested many different autoencoder archetectures we settled on one which produced the highest accuracy among all tested
in order to use in our final netowrk.

Next we wanted to test some data normalization techniques before implementing our dim-reduced model. For this we compared the performances
of our model on non-normalized data, and on data which is pre-processed to be on the range 0,1. Thus instead of total stat and total usage
we avergaged both with their totals to ensure the data was much lower.


25051260 +  22493406 + 18295101 + 12653089 + 15215828 + 12313966 + 17386506 + 35253416 + 13404204 + 16050768 + 16288145 + 16002006
220407695
98700353536/220407695 = 447.8081
220407695 * 0.0001668 = 36764.0035

12833931 + 10865610 + 12442520
36142061
33361537024/36142061 = 923.0668
36142061 * 0.0001410 = 5096.0306